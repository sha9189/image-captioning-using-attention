{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596694596276",
   "display_name": "Python 3.7.6 64-bit ('venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file containing all caption-image pairs\n",
    "with open('Dataset/Flickr30k/Flickr30k.token.txt', 'r') as file:\n",
    "    annotations = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get path to the image folder\n",
    "PATH = os.path.abspath('.') + '/Dataset/Flickr30k/flickr30k-images/'\n",
    "\n",
    "# Store captions and image names in vectors\n",
    "all_captions = []\n",
    "all_img_name_vector = []\n",
    "\n",
    "# splitting the file contents by line\n",
    "for annot in annotations.split(\"\\n\"):\n",
    "        # Skip empty lines\n",
    "        if len(annot)<1:\n",
    "            continue\n",
    "        # separate out the caption from the line\n",
    "        caption = annot.split()[1:]\n",
    "        # add <start> and <end> token to the caption\n",
    "        caption = \"<start> \" + ' '.join(caption) + \" <end>\"\n",
    "        # separate out the image id from line)\n",
    "        image_id = annot.split()[0]\n",
    "        # remove caption number\n",
    "        image_id = image_id.split('#')[0]\n",
    "        # convert image id into the image path\n",
    "        full_image_path = PATH + image_id\n",
    "\n",
    "        all_img_name_vector.append(full_image_path)\n",
    "        all_captions.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(158915,\n '/home/shailesh/Projects/mytf2/Flickr30k_notebooks/Dataset/Flickr30k/flickr30k-images/1000092795.jpg',\n '<start> Two young guys with shaggy hair look at their hands while hanging out in the yard . <end>')"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "len(all_captions), all_img_name_vector[0], all_captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load and preprocess the image input for InceptionV3 pretrained model\n",
    "def load_image(image_path):\n",
    "    \"\"\"loads and preprocesses image for imception-v3 model\n",
    "    input:\n",
    "        image_path ::= string\n",
    "    returns:\n",
    "        img ::= Image tensor of shape (299, 299)\n",
    "        image_path := string\n",
    "    \"\"\"\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels = 3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Inception-V3 model object used for featufre extraction of images\n",
    "image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "3973it [12:29,  5.30it/s]\n"
    }
   ],
   "source": [
    "## TODO: Implement shradding the images for performance enhancement of image caching\n",
    "\n",
    "# Caching the image features to be used while training the model\n",
    "\n",
    "# Get unique images\n",
    "encode_train = sorted(set(all_img_name_vector))\n",
    "\n",
    "# create Dataset object to iterate over all the the image paths\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "# load image using the image paths\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(8)\n",
    "\n",
    "# iterate over the images and store respective features as numpy array\n",
    "for img, path in tqdm(image_dataset):\n",
    "    batch_features = image_features_extract_model(img) # output shape = (?, 8, 8, 2048)\n",
    "    batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3])) # (?, 8, 8, 2048) -> (?, 64, 2048)\n",
    "\n",
    "    for bf, p in zip(batch_features, path):\n",
    "        # get the image path\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "        # change the parent directory in the image path\n",
    "        path_of_feature = path_of_feature.split('/')\n",
    "        path_of_feature[-2] = \"Image_Features\"\n",
    "        path_of_feature = '/'.join(path_of_feature)\n",
    "        # save the features for later use\n",
    "        np.save(path_of_feature, bf.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide images into dev and train set\n",
    "encode_train = sorted(set(all_img_name_vector))\n",
    "\n",
    "encode_train = shuffle(encode_train, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last 2k images be dev set\n",
    "len(encode_train)-2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = open(\"Flickr_30k.trainImages.txt\", 'w')\n",
    "for id in encode_train[:29783]:\n",
    "    id = id.split('/')[-1]\n",
    "    train_file.write(id+'\\n')\n",
    "\n",
    "train_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_file = open(\"Flickr_30k.devImages.txt\", 'w')\n",
    "for id in encode_train[29783:]:\n",
    "    id = id.split('/')[-1]\n",
    "    dev_file.write(id+'\\n')\n",
    "\n",
    "dev_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}